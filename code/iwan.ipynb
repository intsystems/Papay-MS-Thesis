{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80d4ab8-aece-4f4e-8c68-7e3c61db25e6",
        "id": "uFpHAKOkuvX9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "SIREN-WAN initialized: dim=5, N_x=400, N_t=20\n",
            "Iter      0 | L2r: 1.218347 | Loss_u: 5520.2217\n",
            "Iter   1000 | L2r: 0.024462 | Loss_u: 80.2962\n",
            "Iter   2000 | L2r: 0.011046 | Loss_u: 52.6475\n",
            "Iter   3000 | L2r: 0.009118 | Loss_u: 31.8148\n",
            "Iter   4000 | L2r: 0.006979 | Loss_u: 21.5423\n",
            "Iter   5000 | L2r: 0.009720 | Loss_u: 25.4938\n",
            "Iter   6000 | L2r: 0.005586 | Loss_u: 28.4218\n",
            "Iter   7000 | L2r: 0.006581 | Loss_u: 48.0427\n",
            "Iter   8000 | L2r: 0.006446 | Loss_u: 52.7316\n",
            "Iter   9000 | L2r: 0.004918 | Loss_u: 25.9735\n",
            "Iter  10000 | L2r: 0.003650 | Loss_u: 12.6060\n",
            "Iter  11000 | L2r: 0.004307 | Loss_u: 13.8130\n",
            "Iter  12000 | L2r: 0.003552 | Loss_u: 21.7656\n",
            "Iter  13000 | L2r: 0.004089 | Loss_u: 24.3719\n",
            "Iter  14000 | L2r: 0.003108 | Loss_u: 15.9557\n",
            "Iter  15000 | L2r: 0.068099 | Loss_u: 302.5242\n",
            "Iter  16000 | L2r: 0.091491 | Loss_u: 434.0579\n",
            "Iter  17000 | L2r: 0.036763 | Loss_u: 112.4852\n",
            "Iter  18000 | L2r: 0.112068 | Loss_u: 281.8498\n",
            "Iter  19000 | L2r: 0.104328 | Loss_u: 260.5639\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import scipy.io\n",
        "\n",
        "# Устройство\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "os.makedirs('./results_siren/', exist_ok=True)\n",
        "\n",
        "\n",
        "# =============================\n",
        "# SIREN (как в INR.pdf)\n",
        "# =============================\n",
        "class SineLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(\n",
        "                    -np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                    np.sqrt(6 / self.in_features) / self.omega_0\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sin(self.omega_0 * self.linear(x))\n",
        "\n",
        "\n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, hidden_layers, out_features,\n",
        "                 outermost_linear=True, first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "        self.net = nn.ModuleList()\n",
        "        self.net.append(SineLayer(in_features, hidden_features,\n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for _ in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(\n",
        "                    -np.sqrt(6 / hidden_features) / hidden_omega_0,\n",
        "                    np.sqrt(6 / hidden_features) / hidden_omega_0\n",
        "                )\n",
        "                final_linear.bias.fill_(0.0)\n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features,\n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "    def forward(self, coords):\n",
        "        # coords: [N, dim+1], должен иметь requires_grad=True извне\n",
        "        x = coords\n",
        "        for layer in self.net:\n",
        "            x = layer(x)\n",
        "        return x  # [N, 1]\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Сеть v (остаётся MLP)\n",
        "# =============================\n",
        "class NetV(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, layers, hidden_size):\n",
        "        super(NetV, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_size)\n",
        "        self.h_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(layers)])\n",
        "        self.output_layer = nn.Linear(hidden_size, output_dim)\n",
        "        self.activation_tanh = nn.Tanh()\n",
        "        self.activation_softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "        hi = self.activation_tanh(self.input_layer(x))\n",
        "        for i, layer in enumerate(self.h_layers):\n",
        "            if i % 2 == 0:\n",
        "                hi = self.activation_softplus(layer(hi))\n",
        "            else:\n",
        "                hi = torch.sin(layer(hi))\n",
        "        return self.output_layer(hi)\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Основной решатель (SIREN-WAN)\n",
        "# =============================\n",
        "class ParabolicWanPDESolver(nn.Module):\n",
        "    def __init__(self, dim, N_x, N_t, N_bd, file_path,\n",
        "                 beta_int=100.0, beta_intw=500.0, beta_bd=1000.0,\n",
        "                 v_step=1, v_rate=0.015, u_step=1, u_rate=0.0001,\n",
        "                 u_hidden_dim=64, u_hidden_layers=4,\n",
        "                 iteration=20, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.dim = dim\n",
        "        self.t0, self.t1 = -1.0, 1.0\n",
        "        self.low, self.up = -1.0, 1.0\n",
        "        self.iteration = iteration\n",
        "        self.dir = file_path\n",
        "\n",
        "        # Параметры точного решения\n",
        "        self.la = np.pi / 2\n",
        "        self.pho = 2.0\n",
        "        self.mu = self.la**2 - 1\n",
        "\n",
        "        # Размеры сэмплирования\n",
        "        self.N_x = N_x\n",
        "        self.N_t = N_t\n",
        "        self.N_bd = N_bd\n",
        "\n",
        "        # Потери\n",
        "        self.beta_int = beta_int\n",
        "        self.beta_intw = beta_intw\n",
        "        self.beta_bd = beta_bd\n",
        "        self.u_step = u_step\n",
        "        self.v_step = v_step\n",
        "\n",
        "        # Сети — ЗАМЕНА: XNODE → SIREN\n",
        "        self.net_u = Siren(\n",
        "            in_features=dim + 1,\n",
        "            hidden_features=u_hidden_dim,\n",
        "            hidden_layers=u_hidden_layers,\n",
        "            out_features=1,\n",
        "            outermost_linear=True,\n",
        "            first_omega_0=1.0,\n",
        "            hidden_omega_0=1.0\n",
        "        ).to(device)\n",
        "\n",
        "        self.net_v = NetV(dim + 1, 1, layers=6, hidden_size=40).to(device)\n",
        "\n",
        "        self.optimizer_u = torch.optim.Adam(self.net_u.parameters(), lr=u_rate)\n",
        "        self.optimizer_v = torch.optim.Adagrad(self.net_v.parameters(), lr=v_rate)\n",
        "\n",
        "        print(f\"SIREN-WAN initialized: dim={dim}, N_x={N_x}, N_t={N_t}\")\n",
        "\n",
        "    def sample_train(self, N_x, N_t, N_bd):\n",
        "        low, up, t0, t1 = self.low, self.up, self.t0, self.t1\n",
        "\n",
        "        # Сэмплируем пространственные и временные точки отдельно\n",
        "        x_spatial = np.random.uniform(low, up, (N_x, self.dim))  # [N_x, dim]\n",
        "        t_temporal = np.linspace(t0, t1, N_t)                   # [N_t]\n",
        "\n",
        "        # Декартово произведение: [N_x * N_t, dim+1]\n",
        "        x_dm_list = []\n",
        "        for t in t_temporal:\n",
        "            for x in x_spatial:\n",
        "                x_dm_list.append(np.concatenate([x, [t]]))\n",
        "        x_dm_full = np.array(x_dm_list)\n",
        "\n",
        "        # f(x,t) = μu - u²\n",
        "        u_exact = self.pho * np.sin(self.la * x_dm_full[:, 0:1]) * np.exp((self.mu - self.la**2) * x_dm_full[:, -1:])\n",
        "        f_dm = self.mu * u_exact - u_exact**2\n",
        "\n",
        "        # Начальные условия (t = t0)\n",
        "        x_init = np.random.uniform(low, up, (N_bd, self.dim))\n",
        "        t_init = np.full((N_bd, 1), t0)\n",
        "        x_init_full = np.concatenate([x_init, t_init], axis=1)\n",
        "        u_init = self.pho * np.sin(self.la * x_init[:, 0:1]) * np.exp((self.mu - self.la**2) * t0)\n",
        "        u_init = u_init.reshape(-1, 1)\n",
        "\n",
        "        # Конечные точки (t = t1)\n",
        "        x_right = np.random.uniform(low, up, (N_bd, self.dim))\n",
        "        t_right = np.full((N_bd, 1), t1)\n",
        "        x_right_full = np.concatenate([x_right, t_right], axis=1)\n",
        "\n",
        "        # Граничные точки\n",
        "        x_bd_list = []\n",
        "        for i in range(self.dim):\n",
        "            x_bound = np.random.uniform(low, up, (N_bd, self.dim))\n",
        "            t_bound = np.random.uniform(t0, t1, (N_bd, 1))\n",
        "            x_bound[:, i] = up\n",
        "            x_bd_list.append(np.concatenate([x_bound, t_bound], axis=1))\n",
        "            x_bound = np.random.uniform(low, up, (N_bd, self.dim))\n",
        "            x_bound[:, i] = low\n",
        "            x_bd_list.append(np.concatenate([x_bound, t_bound], axis=1))\n",
        "        x_bd = np.concatenate(x_bd_list, axis=0)\n",
        "        u_bd = self.pho * np.sin(self.la * x_bd[:, 0:1]) * np.exp((self.mu - self.la**2) * x_bd[:, -1:])\n",
        "        u_bd = u_bd.reshape(-1, 1)\n",
        "\n",
        "        # В тензоры\n",
        "        to_t = lambda x: torch.FloatTensor(x).to(self.device)\n",
        "        return {\n",
        "            'x_dm': to_t(x_dm_full),\n",
        "            'x_init': to_t(x_init_full),\n",
        "            'x_right': to_t(x_right_full),\n",
        "            'x_bd': to_t(x_bd),\n",
        "            'f_val': to_t(f_dm),\n",
        "            'u_init': to_t(u_init),\n",
        "            'u_bd': to_t(u_bd)\n",
        "        }\n",
        "\n",
        "    def sample_test(self, N_test=5000):\n",
        "        x_test = np.random.uniform(self.low, self.up, (N_test, self.dim))\n",
        "        t_test = np.random.uniform(self.t0, self.t1, (N_test, 1))\n",
        "        x_test_full = np.concatenate([x_test, t_test], axis=1)\n",
        "        u_exact = self.pho * np.sin(self.la * x_test[:, 0:1]) * np.exp((self.mu - self.la**2) * t_test)\n",
        "        u_exact = u_exact.reshape(-1, 1)\n",
        "        return {\n",
        "            'test_x': torch.FloatTensor(x_test_full).to(self.device),\n",
        "            'test_u': torch.FloatTensor(u_exact).to(self.device)\n",
        "        }\n",
        "\n",
        "    def fun_w(self, x):\n",
        "        I1 = 0.210987\n",
        "        x_list = torch.split(x, 1, dim=1)\n",
        "        h_len = (self.up - self.low) / 2.0\n",
        "        x_scale_list = [(x_i - self.low - h_len) / h_len for x_i in x_list]\n",
        "        z_x_list = []\n",
        "        for xs in x_scale_list:\n",
        "            supp = (1 - torch.abs(xs)) > 0\n",
        "            denom = xs ** 2 - 1\n",
        "            safe_denom = torch.where(supp, denom, torch.ones_like(denom))\n",
        "            z_x = torch.where(supp, torch.exp(1.0 / safe_denom) / I1, torch.zeros_like(xs))\n",
        "            z_x_list.append(z_x)\n",
        "        w_val = torch.ones_like(z_x_list[0])\n",
        "        for z in z_x_list:\n",
        "            w_val = w_val * z\n",
        "        if x.requires_grad:\n",
        "            dw = torch.autograd.grad(w_val, x, grad_outputs=torch.ones_like(w_val),\n",
        "                                    create_graph=True, allow_unused=False)[0]\n",
        "            dw = torch.where(torch.isnan(dw), torch.zeros_like(dw), dw)\n",
        "        else:\n",
        "            dw = torch.zeros_like(x)\n",
        "        return w_val, dw\n",
        "\n",
        "    def grad_u(self, x_in):\n",
        "        x_in = x_in.detach().requires_grad_(True)\n",
        "        u_val = self.net_u(x_in)\n",
        "        grad_u = torch.autograd.grad(u_val, x_in, grad_outputs=torch.ones_like(u_val),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "        return u_val, grad_u\n",
        "\n",
        "    def grad_v(self, x_in):\n",
        "        x_in = x_in.detach().requires_grad_(True)\n",
        "        v_val = self.net_v(x_in)\n",
        "        grad_v = torch.autograd.grad(v_val, x_in, grad_outputs=torch.ones_like(v_val),\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "        return v_val, grad_v\n",
        "\n",
        "    def compute_loss(self, train_dict):\n",
        "        x_dm = train_dict['x_dm']\n",
        "        x_init = train_dict['x_init']\n",
        "        x_right = train_dict['x_right']\n",
        "        x_bd = train_dict['x_bd']\n",
        "        f_val = train_dict['f_val']\n",
        "        u_init_true = train_dict['u_init']\n",
        "        u_bd_true = train_dict['u_bd']\n",
        "\n",
        "        u_val, grad_u = self.grad_u(x_dm)\n",
        "        v_val, grad_v = self.grad_v(x_dm)\n",
        "        grad_u_x = grad_u[:, :-1]; grad_u_t = grad_u[:, -1:]\n",
        "        grad_v_x = grad_v[:, :-1]; grad_v_t = grad_v[:, -1:]\n",
        "\n",
        "        w_val, grad_w = self.fun_w(x_dm[:, :-1])\n",
        "        wv_val = w_val * v_val\n",
        "\n",
        "        dudw_val = torch.sum(grad_u_x * grad_w, dim=1, keepdim=True)\n",
        "        dudv_val = torch.sum(grad_u_x * grad_v_x, dim=1, keepdim=True)\n",
        "        dudwv_val = v_val * dudw_val + w_val * dudv_val\n",
        "        u_w_dvt_val = u_val * w_val * grad_v_t\n",
        "        uu_wv_val = u_val * u_val * wv_val\n",
        "        f_wv_val = f_val * wv_val\n",
        "\n",
        "        u_init_pred, _ = self.grad_u(x_init)\n",
        "        u_right_pred, _ = self.grad_u(x_right)\n",
        "        w_init, _ = self.fun_w(x_init[:, :-1])\n",
        "        w_right, _ = self.fun_w(x_right[:, :-1])\n",
        "        v_init, _ = self.grad_v(x_init)\n",
        "        v_right, _ = self.grad_v(x_right)\n",
        "\n",
        "        uwv_init = torch.mean(u_init_pred * w_init * v_init)\n",
        "        uwv_right = torch.mean(u_right_pred * w_right * v_right)\n",
        "\n",
        "        term1 = uwv_right\n",
        "        term2 = torch.mean(dudwv_val)\n",
        "        term3 = torch.mean(u_w_dvt_val)\n",
        "        term4 = uwv_init\n",
        "        term5 = torch.mean(uu_wv_val)\n",
        "        term6 = torch.mean(f_wv_val)\n",
        "\n",
        "        residual = (term1 + term2 - term3) - (term4 + term5 + term6)\n",
        "        test_norm = torch.mean(wv_val**2) + 1e-8\n",
        "        loss_int = self.beta_int * (residual ** 2) / test_norm\n",
        "\n",
        "        # Вспомогательный\n",
        "        uw_init = torch.mean(u_init_pred * w_init)\n",
        "        uw_right = torch.mean(u_right_pred * w_right)\n",
        "        dudw_aux = torch.mean(dudw_val)\n",
        "        uu_w_aux = torch.mean(u_val * u_val * w_val)\n",
        "        f_w_aux = torch.mean(f_val * w_val)\n",
        "        residual_w = (uw_right + dudw_aux) - (uw_init + uu_w_aux + f_w_aux)\n",
        "        w_norm = torch.mean(w_val**2) + 1e-8\n",
        "        loss_intw = self.beta_intw * (residual_w ** 2) / w_norm\n",
        "\n",
        "        loss_bd = torch.mean(torch.abs(self.net_u(x_bd) - u_bd_true))\n",
        "        loss_init = torch.mean(torch.abs(u_init_pred - u_init_true))\n",
        "\n",
        "        loss_u = self.beta_bd * (loss_bd + loss_init) + loss_int + loss_intw\n",
        "        loss_v = -torch.log(loss_int + 1e-8)\n",
        "\n",
        "        return loss_u, loss_v, loss_int, loss_bd, loss_init\n",
        "\n",
        "    def train(self):\n",
        "        history = {'step': [], 'loss_u': [], 'loss_v': [], 'l2r': []}\n",
        "        test_dict = self.sample_test()\n",
        "\n",
        "        for i in range(self.iteration):\n",
        "            train_dict = self.sample_train(self.N_x, self.N_t, self.N_bd)\n",
        "            loss_u, loss_v, loss_int, loss_bd, loss_init = self.compute_loss(train_dict)\n",
        "\n",
        "            # Обучение v\n",
        "            for _ in range(self.v_step):\n",
        "                self.optimizer_v.zero_grad()\n",
        "                _, loss_v, _, _, _ = self.compute_loss(train_dict)\n",
        "                loss_v.backward()\n",
        "                self.optimizer_v.step()\n",
        "\n",
        "            # Обучение u\n",
        "            for _ in range(self.u_step):\n",
        "                self.optimizer_u.zero_grad()\n",
        "                loss_u, _, _, _, _ = self.compute_loss(train_dict)\n",
        "                loss_u.backward()\n",
        "                self.optimizer_u.step()\n",
        "\n",
        "            # Логирование\n",
        "            if i % 100 == 0:\n",
        "                with torch.no_grad():\n",
        "                    pred_u = self.net_u(test_dict['test_x'])\n",
        "                    test_u = test_dict['test_u']\n",
        "                    err_l2 = torch.sqrt(torch.mean((test_u - pred_u)**2)).item()\n",
        "                    u_norm = torch.sqrt(torch.mean(test_u**2)).item()\n",
        "                    l2r = err_l2 / (u_norm + 1e-8)\n",
        "\n",
        "                    history['step'].append(i)\n",
        "                    history['loss_u'].append(loss_u.item())\n",
        "                    history['loss_v'].append(loss_v.item())\n",
        "                    history['l2r'].append(l2r)\n",
        "\n",
        "                if i % 1000 == 0:\n",
        "                    print(f\"Iter {i:6d} | L2r: {l2r:.6f} | Loss_u: {loss_u.item():.4f}\")\n",
        "\n",
        "        return history, test_dict\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Запуск\n",
        "# =============================\n",
        "if __name__ == '__main__':\n",
        "    dim = 5\n",
        "    demo = ParabolicWanPDESolver(\n",
        "        dim=dim,\n",
        "        N_x=400,      # Пространственные точки\n",
        "        N_t=20,       # Временные точки\n",
        "        N_bd=400,     # Граничные точки\n",
        "        file_path='./results_siren/',\n",
        "        beta_int=100.0,\n",
        "        beta_intw=500.0,\n",
        "        beta_bd=1000.0,\n",
        "        v_step=1,\n",
        "        v_rate=0.015,\n",
        "        u_step=1,\n",
        "        u_rate=0.0001,  # 1e-4 для SIREN\n",
        "        u_hidden_dim=64,\n",
        "        u_hidden_layers=4,\n",
        "        iteration=20000,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    history, test_dict = demo.train()\n",
        "\n",
        "    # Сохранение\n",
        "    torch.save(demo.net_u.state_dict(), './results_siren/net_u_siren.pth')\n",
        "    scipy.io.savemat('./results_siren/history_siren.mat', history)\n",
        "    print(\"SIREN-WAN training completed and saved.\")"
      ]
    }
  ]
}